# Unit 2

## Content

- Homework 1
- Homework 2
- Homework 3
- Practice 1
- Practice 2
- Practice 3
- Practice 4
- Practice 5
- Practice 6
- Evaluation
- Colaborators


## Homework 1
1. Regression algorithms
In regression tasks, the machine learning program must estimate and understand the relationships between variables. Regression analysis focuses on one dependent variable and a number of other changing variables, making it particularly useful for forecasting and forecasting.
2. Bayesian algorithms
These types of classification algorithms are based on Bayes' theorem and classify each value as independent of any other. This allows predicting a class or category based on a given set of characteristics, using probability.
Despite its simplicity, the classifier works surprisingly well and is often used because it outperforms more sophisticated classification methods.
3. Grouping algorithms
They are used in unsupervised learning, and serve to categorize unlabelled data, that is, data without defined categories or groups.
The algorithm works by searching for groups within the data, with the number of groups represented by the variable K. Next, it works iteratively to assign each data point to one of the K groups according to the characteristics provided.
4. Decision tree algorithms
A decision tree is a tree structure similar to a flowchart that uses a branching method to illustrate every possible result of a decision. Each node within the tree represents a test on a specific variable, and each branch is the result of that test.
5. Neural network algorithms
An artificial neural network (RNA) comprises units arranged in a series of layers, each of which connects to the adjacent layers. RNAs are inspired by biological systems, such as the brain, and how they process information.
Thus they are essentially a large number of interconnected processing elements, working in unison to solve specific problems.
They also learn by example and experience, and are extremely useful for modeling nonlinear relationships in high-dimensional data, or where the relationship between the input variables is difficult to understand.
6. Dimension reduction algorithms
Dimension reduction reduces the number of variables that are considered to find the exact information required
7. Deep Learning Algorithms
Deep learning algorithms execute data through several layers of neural network algorithms, which move to a simplified representation of the data to the next layer. Most work well on data sets that have up to a few hundred features or columns. However, an unstructured dataset, such as an image, has such a large number of features that this process becomes cumbersome or completely unfeasible.

## Homework 2
**VectorAssembler** is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models such as logistic regression and decision trees. VectorAssembler accepts the following types of input columns: all numeric types, boolean type and vector type. In each row, the values in the input columns will be concatenated into a vector in the specified order.
Root-mean-square deviation.

Example:
Suppose we have a DataFrame with the columns id, hour, mobile, userFeatures and
clicked:
![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/va1.png)

userFeatures is a vector column containing three user functions. We want Combine time, mobile, and user characteristics into a single feature vector called features and use it to predict clicks or not. If we configure the columns of VectorAssembler input in time, mobile and user characteristics and column output in characteristics, after the transformation we should obtain the following DataFrame:

![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/va2.png)

**The root of the root mean square error (RECM) or root of the root mean square deviation (RDCM)** is a frequently used measure of the differences between the values (sample or population values) predicted by a model or estimator and the observed values . The RECM represents the square root of the second moment of the sample of the differences between the predicted values and the observed values or the root mean of these differences. These deviations are called residuals when the calculations are made on the data sample that was used for the estimation and are called errors (or prediction errors) when they are calculated outside the sample.
Formula:

![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/formula2.png)

## Homework 3
**The pipeline** is a technique for implementing instruction-level concurrency within a single processor. Pipelining tries to keep each part of the processor busy, dividing the incoming instructions into a series of sequential steps, which are performed by different processor units that work simultaneously. It increases CPU performance at a certain clock speed, although it can increase latency due to the additional overhead of the pipeline process itself.

![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/pipeline.png)

**CONFUSION MATRIX** It is a tool that allows the visualization of the performance of an algorithm that is used in supervised learning. Each column in the array represents the number of predictions for each class, while each row represents the instances in the actual class. One of the benefits of confusion matrices is that they make it easy to see if the system is confusing two classes. If the number of samples from different classes changes a lot in the input data, the classifier error rate is not representative of how well the classifier performs the task. If for example there are 990 samples from class 1 and only 10 from class 2, the classifier can easily have a bias towards class 1. If the classifier classifies all the samples as class 1, its precision will be 99%. This does not mean that it is a good classifier, since it had a 100% error in classifying class 2 samples.

Example:
Out of 8 real cats, the system predicted that three were dogs and out of six dogs predicted that one was a rabbit and two were cats. From the matrix it can be seen that the system has trouble distinguishing between cats and dogs, but can reasonably well distinguish between rabbits and other animals.

![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/matriz.png)

## Practice 1
Decision trees
Import librarys
```
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.DecisionTreeClassificationModel
import org.apache.spark.ml.classification.DecisionTreeClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")
```
Index labels, adding metadata to the label column.
Fit on whole dataset to include all labels in index.
```
val labelIndexer = new StringIndexer().setInputCol("label").setOutputCol("indexedLabel").fit(data)
```
Automatically identify categorical features, and index them.
```
val featureIndexer = new VectorIndexer().setInputCol("features").setOutputCol("indexedFeatures").setMaxCategories(4) 
```
Features with > 4 distinct values are treated as continuous..fit(data)
Split the data into training and test sets (30% held out for testing).
```
val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))
```
Train a DecisionTree model.
```
val dt = new DecisionTreeClassifier().setLabelCol("indexedLabel").setFeaturesCol("indexedFeatures")
```
Convert indexed labels back to original labels.
```
val labelConverter = new IndexToString().setInputCol("prediction").setOutputCol("predictedLabel").setLabels(labelIndexer.labels)
```
Chain indexers and tree in a Pipeline.
```
val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))
```
Train model. This also runs the indexers.
```
val model = pipeline.fit(trainingData)
```
Make predictions.
```
val predictions = model.transform(testData)
```
Select example rows to display.
```
predictions.select("predictedLabel", "label", "features").show(5)
```
Select (prediction, true label) and compute test error.
```
val evaluator = new MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")
val accuracy = evaluator.evaluate(predictions)
println(s"Test Error = ${(1.0 - accuracy)}") 
val treeModel = model.stages(2).asInstanceOf[DecisionTreeClassificationModel]
println(s"Learned classification tree model:\n ${treeModel.toDebugString}")
```

## Practice 2
Random Forest Classifier
```
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}
```
Load and parse the data file, converting it to a DataFrame.
```
val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")
```
Index labels, adding metadata to the label column.
Fit on whole dataset to include all labels in index.
```
val labelIndexer = new StringIndexer().setInputCol("label").setOutputCol("indexedLabel").fit(data)
```
Automatically identify categorical features, and index them.
Set maxCategories so features with > 4 distinct values are treated as continuous.
```
val featureIndexer = new VectorIndexer().setInputCol("features").setOutputCol("indexedFeatures").setMaxCategories(4).fit(data) 
```
Split the data into training and test sets (30% held out for testing).
```
val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))
```
Train a RandomForest model.
```
val rf = new RandomForestClassifier().setLabelCol("indexedLabel").setFeaturesCol("indexedFeatures").setNumTrees(10)
```
Convert indexed labels back to original labels.
```
val labelConverter = new IndexToString().setInputCol("prediction").setOutputCol("predictedLabel")
setLabels(labelIndexer.labels) 
```
Chain indexers and forest in a Pipeline.
```
val pipeline = new Pipeline()
```

## Practice 3
Gradient-Boosted Tree Classifier
```
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer} 
```
Load and parse the data file, converting it to a DataFrame.
```
val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")
```
Index labels, adding metadata to the label column.
Fit on whole dataset to include all labels in index.
```
val labelIndexer = new StringIndexer().setInputCol("label").setOutputCol("indexedLabel").fit(data)
```
Automatically identify categorical features, and index them.
Set maxCategories so features with > 4 distinct values are treated as continuous.
```
val featureIndexer = new VectorIndexer().setInputCol("features").setOutputCol("indexedFeatures").setMaxCategories(4).fit(data)
```
Split the data into training and test sets (30% held out for testing).
```
val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3)) 
```
Train a GBT model.
```
val gbt = new GBTClassifier().setLabelCol("indexedLabel").setFeaturesCol("indexedFeatures").setMaxIter(10).setFeatureSubsetStrategy("auto")
 ```
Convert indexed labels back to original labels.
```
val labelConverter = new IndexToString().setInputCol("prediction").setOutputCol("predictedLabel").setLabels(labelIndexer.labels)
```
Chain indexers and GBT in a Pipeline.
```
val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, gbt, labelConverter))
```
Train model. This also runs the indexers.
```
val model = pipeline.fit(trainingData)
```
Make predictions.
```
val predictions = model.transform(testData)
```
Select example rows to display.
```
predictions.select("predictedLabel", "label", "features").show(5)
``` 
Select (prediction, true label) and compute test error.
```
val evaluator = new 
MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")
val accuracy = evaluator.evaluate(predictions)
println(s"Test Error = ${1.0 - accuracy}") 
val gbtModel = model.stages(2).asInstanceOf[GBTClassificationModel]
println(s"Learned classification GBT model:\n ${gbtModel.toDebugString}")
```

## Practice 4
Multilayer Perceptron
```
package org.apache.spark.examples.ml
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.sql.SparkSession
```
An example of multilayer perceptron classification.
Step 1: create a Spark session where the class is as follows:
```
object MultilayerPerceptronClassifierExample
 {
  def main(args: Array[String]): Unit = {
	val spark = SparkSession.builder.appName("MultilayerPerceptronClassifierExample").getOrCreate()
```
Step 2: load and analyze the dataset, load the stored data in LIBSVM format as a DataFrame.
Here Spark store is set to "/usr/local/spark-2.3.4-bin-hadoop2.6/data/mllib/s", but you need to configure its path accordingly
```
val data = spark.read.format("libsvm").load("/usr/local/spark-2.3.4-bin-hadoop2.6/data/mllib/sample_multiclass_classification_data.txt")
``` 
Step 3: preparing the training and test set Prepare the train and the test set: training => 60%, test => 40% and seed => 12345L
```
 val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)
	val train = splits(0)
	val test = splits(1) 
```
Step 4: Specify the layers for the neural network Specify the layers for the neural network as follows: input layer => size 4 (characteristics), two intermediate layers (i.e. hidden layer) of size 5 and 4 and output => size 3 (classes).
```
	val layers = Array[Int](4, 5, 4, 3)
```
Step 5: create the MultilayerPerceptronClassifier trainer and set its parameters
```
val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)
```
Step 6: Train the multilayer perceptron classification model using the estimator Train the multilayer perceptron classification model using the estimator above (i.e. trainer)
```	
 val model = trainer.fit(train)
```
Step 7: calculate the accuracy on the test set
```
val result = model.transform(test)
val predictionAndLabels = result.select("prediction", "label")
```
Step 8: evaluate the model for prediction performance
```
	val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
```
Step 9: dummy adjustment if necessary if the classifier performance is low enough. One reason is that the Perceptron is very shallow, the size of the data set is also smaller; therefore, we should continue to try to drill down by at least increasing the size of the hidden layers.
 
```
 println(s"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}")
	// $example off$
  //Paso 10: detener la sesión de Spark
	spark.stop()
  }
}
```

## Practice 5
We import libraries
```
package org.apache.spark.examples.ml
import org.apache.spark.ml.classification.LinearSVC
import org.apache.spark.sql.SparkSession
``` 
Create session spark variable
```
val spark=SparkSession.builder.appName("LinearSVCExample").getOrCreate()
```
We load the data from our file and add it to a variable to train
```
val training=spark.read.format("libsvm").load("/usr/local/spark-2.3.4-bin-hadoop2.6/data/mllib/sample_libsvm_data.txt")
```
We create an object of type LinearSVC, we set the number of iterations to 10 with the setMaxIter method, Set the regularization parameter
```
val lsvc = new LinearSVC().setMaxIter(10).setRegParam(0.1)
```
Fit the model
```
val lsvcModel = lsvc.fit(training)
```
We print the coefficients of the vector and its interception
```
println(s"Coefficients: ${lsvcModel.coefficients} Intercept: ${lsvcModel.intercept}")
```

## Practice 6
We import the libraries and packages necessary to load the program.
```
import  org.apache.spark.ml.classification. { LogisticRegression ,  OneVsRest } 
import  org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.sql.SparkSession
```
Create an instance of the spark session
```
val spark = SparkSession.builder.appName("OneVsRestExample").getOrCreate()
```
Loading our dataset 
```
val Data = spark.read.format("libsvm").load("sample_multiclass_classification_data.txt")
Data.show()
```
Later we carry out our training with our data as follows:
divide the data into training and test sets using an array (20% for testing and 80% training).
```
 val Array(train, test) = inputData.randomSplit(Array(0.8, 0.2))
```
We instantiate the base of the classifier which will contain the maximum of interactions, tolerance and interceptions adjustment.
```
val classifier = new LogisticRegression().setMaxIter(10).setTol(1E-6).setFitIntercept(true)
```
We generate the OneVsRest instances which will bring us the classifier
```
val ovr = new OneVsRest().setClassifier(classifier)
```
Training of the multiclass model generating an adjustment to the training data
```
val ovrModel = ovr.fit(train)
```
We transform the test data into the prediction method
```
val predictions = ovrModel.transform(test)
```
We generate the evaluator which we will bring the instance name of the metric
```
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
```
calculates the classification error in the test data.
```
val accuracy = evaluator.evaluate(predictions)
println(s"Test Error = ${1 - accuracy}")
```

## Practice 7
```
import org.apache.spark.ml.classification.NaiveBayes
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
```
Cargue los datos almacenados en formato LIBSVM como un DataFrame
```
val data = spark.read.format("libsvm").load("data/sample_libsvm_data.txt")
```
Dividir los datos en conjuntos de entrenamiento y prueba (30% para pruebas)
```
val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3), seed = 1234L)
```
Entrenar el modelo naive bayes.
```
val model = new NaiveBayes().fit(trainingData)
```

## Evaluation
We import the libraries to use
```
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier 
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.ml.linalg.Vectors
```
Load our dataset
```
val data = spark.read.format("csv").option("inferSchema","true").option("header","true").csv("iris.csv")
```
Our database is cleaned to eliminate erroneous or null values
```
val Clean = data.na.drop()
```
Show column names
```
Clean.columns
```
![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/dme1.png)

Show the structure of the dataframe
```
Clean.printSchema()
```
![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/dme3.png)

Show the first 5 columns
```
Clean.show(5)
```
![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/dme4.png)

Give general information about dataframe data
```
Clean.describe().show
```
![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/dme5.png)

We generate a vector where the characteristics of the dataset to be evaluated will be stored and saved using the new column features   
```
val assembler = new VectorAssembler().setInputCols(Array("sepal_length","sepal_width","petal_length","petal_width")).setOutputCol("features")
```
We transform the data using our dataset
```
val featureSet = assembler.transform(Clean)
featureSet.show()
```
![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/dme6.png)

We transform the string values ​​of the species column to numerical data to be able to use it
```
val labelIndexer = new StringIndexer().setInputCol("species").setOutputCol("label")
val dataindex = labelIndexer.fit(featureSet).transform(featureSet)
```
show the index of species in column called label
```
dataindex.show()
```
![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/dme7.png)

Data is divided into training data and test data 60% for training and 40% for testing
```
val splits = dataindex.randomSplit(Array(0.6, 0.4), seed = 1234L)
```
take the value of 60% of the data
```
val train = splits(0)
```
take the value of 40% of the data
```
val test = splits(1)
```
We set the layers of our neural network input 4, 5 and 4 intermediate layer or hidden layer and exit 3.
```
val layers = Array[Int](4, 5, 4, 3)
```
We do the data training applying our multilayerPerceptron algorithm
```
val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)
```
the model is trained with the assigned data
```
val model = trainer.fit(train)
```
is tested with the model already trained
```
val result = model.transform(test)
```
select prediction and label to save them in the predictionandlabels variable
```
val predictionAndLabels = result.select("prediction", "label")
```
show the content of the variable
```
predictionAndLabels.show()
```
![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/dme8.png)


The efficiency of the model is calculated
```
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
```
We print the prediction result.
```
println(s"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}")
```

![img](https://github.com/Drani04/DatosMasivos/blob/Unit-2/images/dme9.png)

## Colaborators
